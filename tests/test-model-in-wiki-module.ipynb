{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext cython\n",
    "%reload_ext line_profiler\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..', 'module'))\n",
    "import wiki\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki module\n",
    "\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# topics = ['anatomy', 'biochemistry', 'cognitive science', 'evolutionary biology',\n",
    "#           'genetics', 'immunology', 'molecular biology', 'chemistry', 'biophysics',\n",
    "#           'energy', 'optics', 'earth science', 'geology', 'meteorology']\n",
    "topics = ['earth science']\n",
    "path_saved = '/Users/harangju/Developer/data/wiki/graphs/dated/'\n",
    "networks = {}\n",
    "for topic in topics:\n",
    "    print(topic, end=' ')\n",
    "    networks[topic] = wiki.Net()\n",
    "    networks[topic].load_graph(path_saved + topic + '.pickle')\n",
    "graph = networks[topic].graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = topics[0]\n",
    "graph = networks[topic].graph\n",
    "tfidf = graph.graph['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "dct = pickle.load(open('/Users/harangju/Developer/data/wiki/models/' + 'dict.model','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%cython -f\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cython cimport floating,boundscheck,wraparound\n",
    "from cython.parallel import prange\n",
    "\n",
    "from libc.math cimport fabs\n",
    "\n",
    "np.import_array()\n",
    "\n",
    "@boundscheck(False)  # Deactivate bounds checking\n",
    "@wraparound(False)\n",
    "def cython_manhattan(floating[::1] X_data, int[:] X_indices, int[:] X_indptr,\n",
    "                     floating[::1] Y_data, int[:] Y_indices, int[:] Y_indptr,\n",
    "                     double[:, ::1] D):\n",
    "    \"\"\"Pairwise L1 distances for CSR matrices.\n",
    "    Usage:\n",
    "    >>> D = np.zeros(X.shape[0], Y.shape[0])\n",
    "    >>> cython_manhattan(X.data, X.indices, X.indptr,\n",
    "    ...                  Y.data, Y.indices, Y.indptr,\n",
    "    ...                  D)\n",
    "    \"\"\"\n",
    "    cdef np.npy_intp px, py, i, j, ix, iy\n",
    "    cdef double d = 0.0\n",
    "    \n",
    "    cdef int m = D.shape[0]\n",
    "    cdef int n = D.shape[1]\n",
    "    \n",
    "    with nogil:                          \n",
    "        for px in prange(m):\n",
    "            for py in range(n):\n",
    "                i = X_indptr[px]\n",
    "                j = Y_indptr[py]\n",
    "                d = 0.0\n",
    "                while i < X_indptr[px+1] and j < Y_indptr[py+1]:\n",
    "                    if i < X_indptr[px+1]: ix = X_indices[i]\n",
    "                    if j < Y_indptr[py+1]: iy = Y_indices[j]\n",
    "                    \n",
    "                    if ix==iy:\n",
    "                        d = d+fabs(X_data[i]-Y_data[j])\n",
    "                        i = i+1\n",
    "                        j = j+1\n",
    "                    \n",
    "                    elif ix<iy:\n",
    "                        d = d+fabs(X_data[i])\n",
    "                        i = i+1\n",
    "                    else:\n",
    "                        d = d+fabs(Y_data[j])\n",
    "                        j = j+1\n",
    "                \n",
    "                if i== X_indptr[px+1]:\n",
    "                    while j < Y_indptr[py+1]:\n",
    "                        iy = Y_indices[j]\n",
    "                        d = d+fabs(Y_data[j])\n",
    "                        j = j+1                                            \n",
    "                else:\n",
    "                    while i < X_indptr[px+1]:\n",
    "                        ix = X_indices[i]\n",
    "                        d = d+fabs(X_data[i])\n",
    "                        i = i+1\n",
    "                        \n",
    "                D[px,py] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as skp\n",
    "import sklearn.metrics.pairwise as smp\n",
    "\n",
    "def year_diffs(graph):\n",
    "    return [graph.nodes[node]['year'] - graph.nodes[neighbor]['year']\n",
    "            for node in graph.nodes\n",
    "            for neighbor in list(graph.successors(node))]\n",
    "\n",
    "def neighbor_similarity(graph, tfidf):\n",
    "    nodes = list(graph.nodes)\n",
    "    return [smp.cosine_similarity(tfidf[:,nodes.index(node)].transpose(),\n",
    "                                  tfidf[:,nodes.index(neighbor)].transpose())[0,0]\n",
    "            for node in nodes\n",
    "            for neighbor in list(graph.successors(node))]\n",
    "\n",
    "def non_neighbor_similarity(graph, tfidf):\n",
    "    nodes = list(graph.nodes)\n",
    "    sim = [smp.cosine_similarity(tfidf[:,nodes.index(n1)].transpose(),\n",
    "                                 tfidf[:,nodes.index(n2)].transpose())[0,0]\n",
    "           for n1 in graph.nodes\n",
    "           for n2 in graph.nodes\n",
    "           if (n2 is not n1) and (n2 not in list(graph.neighbors(n1)))]\n",
    "    return sim\n",
    "\n",
    "def sparse_manhattan(X,Y=None):\n",
    "    X, Y = smp.check_pairwise_arrays(X, Y)\n",
    "    X = sp.sparse.csr_matrix(X, copy=False)\n",
    "    Y = sp.sparse.csr_matrix(Y, copy=False)\n",
    "    res = np.empty(shape=(X.shape[0],Y.shape[0]))\n",
    "    cython_manhattan(X.data,X.indices,X.indptr,\n",
    "                     Y.data,Y.indices,Y.indptr,\n",
    "                             res)\n",
    "    return res\n",
    "\n",
    "def word_diffs(graph, tfidf):\n",
    "    dists = sparse_manhattan(X=skp.binarize(tfidf).transpose())\n",
    "    nodes = list(graph.nodes)\n",
    "    return [dists[nodes.index(node), nodes.index(neighbor)]\n",
    "            for node in nodes\n",
    "            for neighbor in list(graph.successors(node))]\n",
    "\n",
    "def weight_differences(graph, tfidf):\n",
    "    nodes = list(graph.nodes)\n",
    "    diff = []\n",
    "    for node in nodes:\n",
    "        for neighbor in graph.successors(node):\n",
    "            v1 = tfidf[:,nodes.index(node)]\n",
    "            v2 = tfidf[:,nodes.index(neighbor)]\n",
    "            idx = np.concatenate([v1.indices, v2.indices])\n",
    "            diff.append( np.sum(np.absolute(v1[idx]-v2[idx])) )\n",
    "    return diff\n",
    "\n",
    "def plot_distribution(data):\n",
    "    bins = np.logspace(np.log10(min(data)), np.log10(max(data)), 30)\n",
    "    hist, edges = np.histogram(data, bins=bins)\n",
    "#     hist_norm = hist/(bins[1:] - bins[:-1])\n",
    "    sns.scatterplot(bins[:-1], hist/len(data))\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(bins[0]/2, bins[-1]*2)\n",
    "    plt.ylim(min(hist[hist>0])/len(data)/2, 1)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('P(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors\n",
    "\n",
    "Run **Auxiliary methods** below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: power law distributions of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import powerlaw\n",
    "# tfidf = graph.graph['tfidf']\n",
    "# fit = powerlaw.Fit(tfidf.data)\n",
    "fit.plot_pdf()\n",
    "fit.power_law.plot_pdf();\n",
    "plt.title(f\"xmin={fit.xmin:.1e}, α={fit.alpha:.1f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: similarity / year between neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,24))\n",
    "plt.subplot(421)\n",
    "yd = year_diffs(graph)\n",
    "sns.distplot(yd)\n",
    "plt.xlabel('Δyear')\n",
    "plt.ylabel('distribution')\n",
    "\n",
    "# wd = word_diffs(graph, tfidf)\n",
    "mu, std = sp.stats.norm.fit(wd)\n",
    "plt.subplot(423)\n",
    "sns.distplot(wd)\n",
    "x = np.linspace(min(wd), max(wd), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "plt.legend([f\"m={mu:.2f}; s={std:.2f}\"])\n",
    "plt.xlabel('manhattan distance')\n",
    "plt.ylabel('distribution');\n",
    "\n",
    "slope, intercept, fit_r, p, stderr = sp.stats.linregress(np.abs(yd), wd)\n",
    "plt.subplot(424)\n",
    "wd = word_diffs(graph, tfidf)\n",
    "sns.scatterplot(x=np.abs(yd), y=wd)\n",
    "x = np.linspace(0, max(yd), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"r={fit_r:.2f}; p={p:.1e}\")\n",
    "plt.legend([f\"slope={slope:.2f}\"])\n",
    "plt.xlabel('Δyear')\n",
    "plt.ylabel('manhattan distance');\n",
    "\n",
    "neighbors = neighbor_similarity(graph, tfidf)\n",
    "# non_neighbors = non_neighbor_similarity(graph, tfidf)\n",
    "fit_mu, fit_std = sp.stats.norm.fit(neighbors)\n",
    "plt.subplot(425)\n",
    "sns.distplot(neighbors, hist=True)\n",
    "x = np.linspace(min(neighbors), max(neighbors), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, fit_mu, fit_std))\n",
    "sns.distplot(non_neighbors)\n",
    "plt.legend([f\"fit-neighbors (m={fit_mu:.2f}; s={fit_std:.2f})\", 'neighbors', 'non-neighbors'])\n",
    "plt.xlabel('cos similarity');\n",
    "\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd), neighbors)\n",
    "plt.subplot(426)\n",
    "neighbors = neighbor_similarity(graph, tfidf)\n",
    "sns.scatterplot(x=np.abs(yd), y=neighbors)\n",
    "x = np.linspace(0, max(yd), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"r={r:.2f}; p={p:.1f}\")\n",
    "plt.legend([f\"slope={slope:.2f}\"])\n",
    "plt.xlabel('Δyear')\n",
    "plt.ylabel('cosine similarity');\n",
    "\n",
    "weight_diffs = weight_differences(graph, tfidf)\n",
    "mu, std = sp.stats.norm.fit(weight_diffs)\n",
    "plt.subplot(427)\n",
    "sns.distplot(weight_diffs)\n",
    "x = np.linspace(min(weight_diffs), max(weight_diffs), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "plt.legend([f\"m={mu:.2f}; s={std:.2f}\"])\n",
    "plt.xlabel('Σ abs Δw_i')\n",
    "plt.ylabel('distribution');\n",
    "\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd), weight_diffs)\n",
    "plt.subplot(428)\n",
    "sns.scatterplot(x=np.abs(yd), y=weight_diffs)\n",
    "x = np.linspace(0, max(yd), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"r={r:.2f}; p={p:.1e}\")\n",
    "plt.legend([f\"slope={slope:.1e}\"])\n",
    "plt.xlabel('Δyear')\n",
    "plt.ylabel('Σ abs Δw_i');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: weight distributions of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(x='index', y='weight',\n",
    "                data=pd.DataFrame({'index': tfidf.indices,\n",
    "                                   'weight': tfidf.data}))\n",
    "sns.scatterplot(x='index', y='weight',\n",
    "                data=pd.DataFrame({'index': tfidf.indices,\n",
    "                                   'weight': tfidf.data})\\\n",
    "                       .groupby('index').mean()\\\n",
    "                       .reset_index())\n",
    "plt.legend(['weights', 'averaged'])\n",
    "plt.ylim([-.2,1.2])\n",
    "plt.subplot(122)\n",
    "plot_distribution(tfidf.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: year distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "bin_size=25\n",
    "years = [graph.nodes[node]['year'] for node in graph.nodes]\n",
    "sns.distplot(years, bins=bin_size, rug=True, kde=False)\n",
    "hist, bin_edges = np.histogram(years, bins=bin_size)\n",
    "popt, pcov = sp.optimize.curve_fit(lambda x,a,b: a*pow(b,x), bin_edges[1:], hist)\n",
    "x = np.linspace(min(years), max(years), 100)\n",
    "sns.lineplot(x=x, y=popt[0]*pow(popt[1],x))\n",
    "plt.legend([f\"a*b^x; a={popt[0]:.1e}, b={popt[1]:.2f}\"])\n",
    "plt.xlabel('year');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: word weight vs title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stoplist=set('for a of the and to in'.split())\n",
    "nodes = []\n",
    "words = []\n",
    "for i in range(tfidf.shape[1]):\n",
    "    node = list(graph.nodes)[i]\n",
    "    if tfidf[:,i].data.size == 0:\n",
    "        print(node, tfidf[:,i].data)\n",
    "        continue\n",
    "    top_words, idx = wiki.Model.find_top_words(tfidf[:,i], dct, top_n=5)\n",
    "    nodes += [node]\n",
    "    words += [top_words]\n",
    "pd.DataFrame(data={'Node': nodes, 'Top words': words})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = tfidf[:,0].copy()\n",
    "y = tfidf[:,0].copy()\n",
    "T = 300\n",
    "\n",
    "sim = np.zeros(T)\n",
    "size = np.zeros(T)\n",
    "mag = np.zeros(T)\n",
    "for i in range(sim.size):\n",
    "    sim[i] = smp.cosine_similarity(x.transpose(),y.transpose())[0,0]\n",
    "    size[i] = y.size\n",
    "    mag[i] = np.sum(y.data)\n",
    "    y = wiki.Model.mutate(y, lambda n: fit.power_law.generate_random(n),\n",
    "                          point=(1,.5), insert=(1,.3,None), delete=(1,.3))\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "ax = plt.subplot(221)\n",
    "sns.lineplot(x=range(sim.size), y=sim)\n",
    "plt.ylabel('similarity')\n",
    "# ax2 = ax.twinx()\n",
    "# sns.lineplot(x=range(sim.size), y=mag, ax=ax2, color='darkorange')\n",
    "# plt.ylabel('magnitude')\n",
    "plt.xlabel('years')\n",
    "\n",
    "plt.subplot(222)\n",
    "sns.lineplot(x=range(sim.size), y=size)\n",
    "plt.ylabel('size')\n",
    "plt.xlabel('years')\n",
    "\n",
    "plt.subplot(223)\n",
    "plot_distribution(x.data)\n",
    "plot_distribution(y.data)\n",
    "plt.xlabel('tf-idf values')\n",
    "plt.legend(['before mutation', 'after mutation'])\n",
    "plt.xlabel('tf-idf values')\n",
    "\n",
    "plt.subplot(224)\n",
    "plot_distribution(x.data)\n",
    "plot_distribution(y.data)\n",
    "plt.xlabel('tf-idf values')\n",
    "plt.yscale('linear')\n",
    "plt.xscale('linear')\n",
    "plt.ylim([0,.2])\n",
    "plt.xlim([0,.1])\n",
    "plt.legend(['before mutation','after mutation']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wiki.Model(graph_parent=networks[topic].graph,\n",
    "                   vectors_parent=networks[topic].graph.graph['tfidf'],\n",
    "                   year_start=-500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_graph = model.graph.copy()\n",
    "test_vector = sp.sparse.hstack([tfidf[:,list(graph.nodes).index(n)] for n in test_graph.nodes])\n",
    "\n",
    "seed = 'Meteorology'\n",
    "seed_vector = tfidf[:,list(graph.nodes).index(seed)]\n",
    "\n",
    "print('Nodes:', test_graph.nodes)\n",
    "print('Edges:', test_graph.edges, '\\n')\n",
    "print(f\"Seed: {seed}\\n\")\n",
    "wiki.Model.connect(seed_vector, test_graph, test_vector, dct, match_n=3)\n",
    "print('Nodes:', test_graph.nodes)\n",
    "print('Edges:', test_graph.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nodes = list(graph.nodes)[:20]\n",
    "seeds = {node: [tfidf[:,list(graph.nodes).index(node)],\n",
    "                tfidf[:,list(graph.nodes).index(node)]]\n",
    "         for node in nodes}\n",
    "print(nodes, '\\n')\n",
    "vectors = sp.sparse.hstack([v for node in nodes for v in seeds[node]])\n",
    "print(np.triu(smp.cosine_similarity(vectors.transpose())))\n",
    "wiki.Model.crossover_seeds(seeds, graph, tfidf, threshold=0.5)\n",
    "print('\\n----------------------------------------------------------\\n')\n",
    "vectors = sp.sparse.hstack([v for node in nodes if node in seeds.keys() for v in seeds[node]])\n",
    "print(np.triu(smp.cosine_similarity(vectors.transpose())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = wiki.Model(graph_parent=networks[topic].graph,\n",
    "                   vectors_parent=networks[topic].graph.graph['tfidf'],\n",
    "                   year_start=-500)\n",
    "print(f\"Topic: '{graph.name}'\\n\" +\\\n",
    "      f\"Core nodes\\n\" +\\\n",
    "      f\"   {list(model.graph.nodes)}\\n\" +\\\n",
    "      f\"Parameters\\n\" +\\\n",
    "      f\"   α (power law): {fit.alpha:.2f}\\n\" +\\\n",
    "      f\"   p_insert/delete: {fit_r:.2f}/2\\n\" +\\\n",
    "      f\"   neighbor_mu, std: {fit_mu:.2f}, {fit_std:.2f}\\n\" +\\\n",
    "      f\"   crossover threshold: {fit_mu+3*fit_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_val = np.max(tfidf.data)\n",
    "%lprun -f model.evolve model.evolve(year_end=2000,\\\n",
    "                                    n_seeds=2,\\\n",
    "                                    point=(1, 2*fit_std),\\\n",
    "                                    insert=(1, fit_r/2, list(set(tfidf.indices))),\\\n",
    "                                    delete=(1, fit_r/2),\\\n",
    "                                    rvs=lambda n: np.vectorize(lambda x: max_val if x>max_val else x,\\\n",
    "                                                               otypes=[np.float64])\\\n",
    "                                                              (fit.power_law.generate_random(n)),\\\n",
    "                                    dct=dct, create=lambda n: np.random.normal(loc=fit_mu+fit_std,\\\n",
    "                                                                               scale=fit_std, size=n),\\\n",
    "                                    crossover=fit_mu+3*fit_std)\n",
    "model.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = lambda a,b: smp.cosine_similarity(a.transpose(), b.transpose())[0,0]\n",
    "nodes = list(model.graph.nodes)\n",
    "model.record['Similarity to parent'] = [s(model.record.iloc[i]['Seed vectors'],\n",
    "                                          model.vectors[:,nodes.index(model.record.iloc[i]['Parent'])])\n",
    "                                        for i in range(len(model.record.index))]\n",
    "model.record['Parent seed'] = model.record['Parent'] + ' ' + model.record['Seed number'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.lineplot(x='Year', y='Similarity to parent', hue='Parent seed', data=model.record);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save/load graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph.graph['tfidf'] = vectors\n",
    "nx.write_gpickle(subgraph, f\"graph{}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = nx.read_gpickle('graph.pickle')\n",
    "vectors = subgraph.graph['tfidf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(121)\n",
    "sns.distplot(neighbors)\n",
    "x = np.linspace(min(neighbors), max(neighbors), 100)\n",
    "mu, std = sp.stats.norm.fit(neighbors)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "sns.distplot(non_neighbors)\n",
    "plt.title(topic + ' (prior)')\n",
    "plt.legend([f\"fit-neighbors (m={mu:.2f}; s={std:.2f})\", 'neighbors', 'non-neighbors'])\n",
    "plt.xlabel('cos similarity');\n",
    "plt.xlim([-.2,1.2])\n",
    "plt.subplot(122)\n",
    "neighbors_model = neighbor_similarity(subgraph, vectors)\n",
    "non_neighbors_model = non_neighbor_similarity(subgraph, vectors)\n",
    "sns.distplot(neighbors_model)\n",
    "x = np.linspace(min(neighbors_model), max(neighbors_model), 100)\n",
    "mu, std = sp.stats.norm.fit(neighbors_model)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "sns.distplot(non_neighbors_model)\n",
    "plt.title(topic + ' (model)')\n",
    "plt.legend([f\"fit-neighbors (m={mu:.2f}; s={std:.2f})\", 'neighbors', 'non-neighbors'])\n",
    "plt.xlabel('cos similarity')\n",
    "plt.xlim([-.2,1.2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(121)\n",
    "sns.distplot([graph.nodes[node]['year'] for node in graph.nodes], rug=True)\n",
    "plt.xlim([-5000,2100])\n",
    "plt.title('prior')\n",
    "plt.ylabel('discoveries')\n",
    "plt.xlabel('year')\n",
    "plt.subplot(122)\n",
    "sns.distplot([subgraph.nodes[node]['year'] for node in subgraph.nodes], rug=True)\n",
    "plt.xlim([-5000,3100])\n",
    "plt.title('model')\n",
    "plt.ylabel('discoveries')\n",
    "plt.xlabel('year');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "fit.plot_pdf()\n",
    "fit.power_law.plot_pdf()\n",
    "plt.title(f\"empirical xmin={fit.xmin:.1e}, α={fit.alpha:.1f}\");\n",
    "plt.subplot(122)\n",
    "fit_model = powerlaw.Fit(vectors.data)\n",
    "fit_model.plot_pdf()\n",
    "fit_model.power_law.plot_pdf()\n",
    "plt.title(f\"model xmin={fit_model.xmin:.1e}, α={fit_model.alpha:.1f}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(121)\n",
    "sns.distplot(yd)\n",
    "plt.title(topic + ' prior')\n",
    "plt.xlabel('year difference')\n",
    "plt.subplot(122)\n",
    "yd_model = year_diffs(subgraph)\n",
    "sns.distplot(yd_model)\n",
    "plt.title(topic + ' model')\n",
    "plt.xlabel('year difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(221)\n",
    "sns.scatterplot(x=np.abs(yd), y=wd)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd), wd)\n",
    "x = np.linspace(0, max(yd), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.2f}; r={r:.2f}; p={p:.1e} (prior)\")\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('manhattan distance');\n",
    "\n",
    "plt.subplot(222)\n",
    "sns.distplot(wd)\n",
    "mu, std = sp.stats.norm.fit(wd)\n",
    "x = np.linspace(min(wd), max(wd), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "plt.xlabel('manhattan distance')\n",
    "plt.ylabel('probability distribution');\n",
    "plt.title(f\"μ={mu:.2}, σ={std:.2} (prior)\")\n",
    "\n",
    "wd_model = word_diffs(subgraph, vectors)\n",
    "\n",
    "plt.subplot(223)\n",
    "sns.scatterplot(x=np.abs(yd_model), y=wd_model)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd_model), wd_model)\n",
    "x = np.linspace(0, max(yd_model), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.2f}; r={r:.2f}; p={p:.1e} (model)\")\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('manhattan distance');\n",
    "\n",
    "plt.subplot(224)\n",
    "sns.distplot(wd_model)\n",
    "mu, std = sp.stats.norm.fit(wd_model)\n",
    "x = np.linspace(min(wd_model), max(wd_model), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "plt.xlabel('manhattan distance')\n",
    "plt.ylabel('probability distribution');\n",
    "plt.title(f\"μ={mu:.2}, σ={std:.2} (model)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_model = neighbor_similarity(subgraph, vectors)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(x=np.abs(yd), y=neighbors)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd), neighbors)\n",
    "x = np.linspace(0, max(yd), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.2f}; r={r:.2f}; p={p:.1e} (prior)\")\n",
    "plt.xlabel('Δyear')\n",
    "plt.ylabel('cosine similarity');\n",
    "plt.subplot(122)\n",
    "sns.scatterplot(x=np.abs(yd_model), y=neighbors_model)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd_model), neighbors_model)\n",
    "x = np.linspace(0, max(yd_model), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.2f}; r={r:.2f}; p={p:.1e} (model)\")\n",
    "plt.xlabel('Δyear')\n",
    "plt.ylabel('cosine similarity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(x='index', y='weight',\n",
    "                data=pd.DataFrame({'index': vectors.indices,\n",
    "                                   'weight': vectors.data}))\n",
    "plt.ylim([-.1,1.1]);\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_distribution(vectors.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(121)\n",
    "nx.draw_networkx(graph, node_color=['r' if graph.nodes[n]['year']<start_year else 'b'\n",
    "                                    for n in graph.nodes])\n",
    "plt.title('original graph')\n",
    "plt.subplot(122)\n",
    "nx.draw_networkx(subgraph, node_color=['r' if subgraph.nodes[n]['year']<start_year else 'b'\n",
    "                                       for n in subgraph.nodes])\n",
    "plt.title('new graph');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The point of this model is that one can model knowledge discovery as incremental changes on existing knowledge.\n",
    "\n",
    "The mutation model doesn't monotonically decrease similarity with parent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
