{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..', 'module'))\n",
    "import wiki\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "### Condensed sparse column matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "row = np.array([0, 2, 2, 0, 1, 2])\n",
    "col = np.array([0, 0, 1, 2, 2, 2])\n",
    "sp.sparse.csc_matrix((data, (row, col)), shape=(3, 3)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = ['anatomy', 'biochemistry', 'cognitive science', 'evolutionary biology',\n",
    "#           'genetics', 'immunology', 'molecular biology', 'chemistry', 'biophysics',\n",
    "#           'energy', 'optics', 'earth science', 'geology', 'meteorology']\n",
    "topics = ['earth science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = '/Users/harangju/Developer/data/wiki/graphs/dated/'\n",
    "networks = {}\n",
    "for topic in topics:\n",
    "    print(topic, end=' ')\n",
    "    networks[topic] = wiki.Net()\n",
    "    networks[topic].load_graph(path_saved + topic + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(networks[topic].graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = networks[topic].graph.graph['tfidf']\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[:,0].indices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks[topic].graph.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks[topic].graph.nodes['Biology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = [n for n in networks[topic].graph.nodes if networks[topic].graph.nodes[n]['core_rb']>.9]\n",
    "core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,n) for i,n in enumerate(networks[topic].graph.nodes) if networks[topic].graph.nodes[n]['year']<-1800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = v[:,9]\n",
    "vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSC & networkx operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = networks[topic].graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = [n for n in networks[topic].graph.nodes if networks[topic].graph.nodes[n]['year']<-2000]\n",
    "subgraph = graph.subgraph(core).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = ss.hstack([v[:,list(graph.nodes).index(n)] for n in subgraph.nodes])\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph.add_node('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "Initialize with core set of nodes.\\\n",
    "For each year,\\\n",
    "initialize an \"baby\" node for each existing node that doesn't already have a baby node,\\\n",
    "mutate tf-idf for each \"baby\" node (including the name),\\\n",
    "and if the \"baby\" node gets a probability drawn from the distribution of similarities (to what?),\n",
    "the \"baby\" node is born."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics.pairwise as smp\n",
    "import scipy.sparse as ss\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: power law distributions of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "graph = networks[topic].graph\n",
    "tfidf = graph.graph['tfidf']\n",
    "x = tfidf[:,1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "fit = powerlaw.Fit(x.data)\n",
    "fit.xmin, fit.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.plot_pdf()\n",
    "fit.power_law.plot_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: new words / year between neighbors\n",
    "[gist](https://gist.github.com/ptocca/e18a9e4e35930c0958fdaa62958bdf82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = networks[topic].graph\n",
    "year_diffs = [[graph.nodes[node]['year'] - graph.nodes[neighbor]['year']\n",
    "               for neighbor in list(graph.successors(node))]\n",
    "               # + list(graph.predecessors(node))]\n",
    "              for node in graph.nodes]\n",
    "year_diffs = [y for ys in year_diffs for y in ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(year_diffs)\n",
    "plt.title(topic)\n",
    "plt.xlabel('year difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as skp\n",
    "import sklearn.metrics.pairwise as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skp.binarize(tfidf[:,0:2]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -f\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cython cimport floating,boundscheck,wraparound\n",
    "from cython.parallel import prange\n",
    "\n",
    "from libc.math cimport fabs\n",
    "\n",
    "np.import_array()\n",
    "\n",
    "#@boundscheck(False)  # Deactivate bounds checking\n",
    "@wraparound(False)\n",
    "def cython_manhattan(floating[::1] X_data, int[:] X_indices, int[:] X_indptr,\n",
    "                     floating[::1] Y_data, int[:] Y_indices, int[:] Y_indptr,\n",
    "                     double[:, ::1] D):\n",
    "    \"\"\"Pairwise L1 distances for CSR matrices.\n",
    "    Usage:\n",
    "    >>> D = np.zeros(X.shape[0], Y.shape[0])\n",
    "    >>> cython_manhattan(X.data, X.indices, X.indptr,\n",
    "    ...                  Y.data, Y.indices, Y.indptr,\n",
    "    ...                  D)\n",
    "    \"\"\"\n",
    "    cdef np.npy_intp px, py, i, j, ix, iy\n",
    "    cdef double d = 0.0\n",
    "    \n",
    "    cdef int m = D.shape[0]\n",
    "    cdef int n = D.shape[1]\n",
    "    \n",
    "    with nogil:                          \n",
    "        for px in prange(m):\n",
    "            for py in range(n):\n",
    "                i = X_indptr[px]\n",
    "                j = Y_indptr[py]\n",
    "                d = 0.0\n",
    "                while i < X_indptr[px+1] and j < Y_indptr[py+1]:\n",
    "                    if i < X_indptr[px+1]: ix = X_indices[i]\n",
    "                    if j < Y_indptr[py+1]: iy = Y_indices[j]\n",
    "                    \n",
    "                    if ix==iy:\n",
    "                        d = d+fabs(X_data[i]-Y_data[j])\n",
    "                        i = i+1\n",
    "                        j = j+1\n",
    "                    \n",
    "                    elif ix<iy:\n",
    "                        d = d+fabs(X_data[i])\n",
    "                        i = i+1\n",
    "                    else:\n",
    "                        d = d+fabs(Y_data[j])\n",
    "                        j = j+1\n",
    "                \n",
    "                if i== X_indptr[px+1]:\n",
    "                    while j < Y_indptr[py+1]:\n",
    "                        iy = Y_indices[j]\n",
    "                        d = d+fabs(Y_data[j])\n",
    "                        j = j+1                                            \n",
    "                else:\n",
    "                    while i < X_indptr[px+1]:\n",
    "                        ix = X_indices[i]\n",
    "                        d = d+fabs(X_data[i])\n",
    "                        i = i+1\n",
    "                        \n",
    "                D[px,py] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix,random\n",
    "from sklearn.metrics.pairwise import check_pairwise_arrays\n",
    "def sparse_manhattan(X,Y=None):\n",
    "    X, Y = check_pairwise_arrays(X, Y)\n",
    "    X = csr_matrix(X, copy=False)\n",
    "    Y = csr_matrix(Y, copy=False)\n",
    "    res = np.empty(shape=(X.shape[0],Y.shape[0]))\n",
    "    cython_manhattan(X.data,X.indices,X.indptr,\n",
    "                     Y.data,Y.indices,Y.indptr,\n",
    "                             res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = sparse_manhattan(X=sp.binarize(tfidf).transpose())\n",
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(graph.nodes)\n",
    "word_diffs = [[dists[nodes.index(node), nodes.index(neighbor)]\n",
    "               for neighbor in list(graph.successors(node))]\n",
    "              for node in graph.nodes]\n",
    "word_diffs = [w for ws in word_diffs for w in ws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=np.abs(year_diffs), y=word_diffs)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(year_diffs), word_diffs)\n",
    "x = np.linspace(0, max(year_diffs), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.4f}; r={r:.4f}; p={p:.4f}\")\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('manhattan distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(word_diffs)\n",
    "mu, std = sp.stats.norm.fit(word_diffs)\n",
    "x = np.linspace(min(word_diffs), max(word_diffs), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "plt.xlabel('manhattan distance')\n",
    "plt.ylabel('probability distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: similarity / year between neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(graph.nodes)\n",
    "neighbors = [[smp.cosine_similarity(v[:,nodes.index(node)].transpose(),\n",
    "                                    v[:,nodes.index(neighbor)].transpose())[0,0]\n",
    "              for neighbor in list(graph.successors(node))]# + list(graph.predecessors(node))]\n",
    "             for node in nodes]\n",
    "neighbors = [s for ss in neighbors for s in ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=np.abs(year_diffs), y=neighbors)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(year_diffs), neighbors)\n",
    "x = np.linspace(0, max(year_diffs), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.4f}; r={r:.4f}; p={p:.4f}\")\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "\n",
    "def mutate(x, rvs=lambda: npr.rand(), p_insert=.3, p_delete=.3, p_point=1):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: spipy.sparse.csc_matrix\n",
    "    rvs: lambda ()-> float\n",
    "        returns a random distance\n",
    "    \"\"\"\n",
    "    data = x.data\n",
    "    idx = x.indices\n",
    "    if npr.rand() < p_point:\n",
    "        data[npr.choice(x.size)] = rvs()\n",
    "    if npr.rand() < p_delete:\n",
    "        delete_idx = npr.choice(idx.size)\n",
    "        idx = np.delete(idx, delete_idx)\n",
    "        data = np.delete(data, delete_idx)\n",
    "    if npr.rand() < p_insert:\n",
    "        while True:\n",
    "            insert_idx = npr.choice(x.shape[0])\n",
    "            if insert_idx not in idx: break\n",
    "        idx = np.append(idx, insert_idx)\n",
    "        data = np.append(data, rvs())\n",
    "    y = ss.csc_matrix((data, (idx, np.zeros(idx.shape, dtype=int))),\n",
    "                      shape=x.shape)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tfidf[:,1].copy()\n",
    "y = tfidf[:,1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T = 100\n",
    "sim = np.zeros(T)\n",
    "size = np.zeros(T)\n",
    "for i in range(sim.size):\n",
    "    sim[i] = smp.cosine_similarity(x.transpose(),y.transpose())[0,0]\n",
    "    size[i] = y.size\n",
    "    y = mutate(y, lambda: fit.power_law.generate_random()[0])\n",
    "plt.figure()\n",
    "sn.lineplot(x=range(sim.size), y=sim)\n",
    "plt.title(graph.name)\n",
    "plt.ylabel('similarity')\n",
    "plt.xlabel('years');\n",
    "plt.figure()\n",
    "sn.lineplot(x=range(sim.size), y=size)\n",
    "plt.title(graph.name)\n",
    "plt.ylabel('size')\n",
    "plt.xlabel('years');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_distribution(data):\n",
    "    plt.figure()\n",
    "    bins = np.logspace(np.log10(min(data)), np.log10(max(data)), 20)\n",
    "    hist, edges = np.histogram(data, bins=bins)\n",
    "#     hist_norm = hist/(bins[1:] - bins[:-1])\n",
    "    plt.plot(bins[:-1], hist/len(data), '.')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(min(hist[hist>0])/len(data)/2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(graph.graph['tfidf'][:,1].data)\n",
    "plt.title(graph.name + ' before mutation')\n",
    "plt.xlabel('tf-idf values');\n",
    "plot_distribution(y.data)\n",
    "plt.title(graph.name + ' after mutation')\n",
    "plt.xlabel('tf-idf values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new nodes\n",
    "\n",
    "#### Prior: distribution of similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = [[smp.cosine_similarity(v[:,list(graph.nodes).index(node)].transpose(),\n",
    "                                    v[:,list(graph.nodes).index(neighbor)].transpose())[0,0]\n",
    "              for neighbor in list(graph.successors(node))\n",
    "              if neighbor is not node]# + list(graph.predecessors(node))]\n",
    "             for node in graph.nodes]\n",
    "neighbors = [s for ss in neighbors for s in ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_neighbors = [[smp.cosine_similarity(v[:,list(graph.nodes).index(n1)].transpose(),\n",
    "                                        v[:,list(graph.nodes).index(n2)].transpose())[0,0]\n",
    "                  for n2 in graph.nodes\n",
    "                  if (n2 is not n1) and (n2 not in list(graph.neighbors(n1)))]\n",
    "                 for n1 in graph.nodes]\n",
    "non_neighbors = [s for ss in non_neighbors for s in ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, std = sp.stats.norm.fit(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.distplot(neighbors)\n",
    "x = np.linspace(min(neighbors), max(neighbors), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "sns.distplot(non_neighbors)\n",
    "plt.title(topic)\n",
    "plt.legend([f\"fit-neighbors (m={mu:.2f}; s={std:.2f})\", 'neighbors', 'non-neighbors'])\n",
    "plt.xlabel('cos similarity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "Just draw from normal pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.normal(loc=mu, scale=std, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossover\n",
    "\n",
    "What prior should I use? It needs to be more similar than neighbors. Some kind of a t-test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: maybe just 3 std above mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu + 3*std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "average? or combine elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tfidf[:,0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ndarray(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.shape, (tfidf.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(scientists):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scientists: \n",
    "    \"\"\"\n",
    "    idx = np.ndarray(0)\n",
    "    data = np.ndarray(0)\n",
    "    for scientist in scientists:\n",
    "        pass\n",
    "#         idx = np.append(idx, npr.choice())\n",
    "#         data = np.append(data, npr.choice())\n",
    "    y = ss.csc_matrix((data, (idx, np.zeros(idx.shape, dtype=int))),\n",
    "                      shape=(scientists.shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get words from tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim.utils as gu\n",
    "\n",
    "path_models = '/Users/harangju/Developer/data/wiki/models/'\n",
    "model = gu.SaveLoad.load(path_models + 'tfidf.model')\n",
    "dct = pickle.load(open(path_models + 'dict.model','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [dct[i] for i in tfidf[:,0].indices]\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: word weight vs title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(tfidf.shape[1]):\n",
    "    idx_max = np.argmax(tfidf[:,i].data)\n",
    "    idx = tfidf[:,i].indices[idx_max]\n",
    "    word = dct[idx]\n",
    "    node = list(graph.nodes)[i]\n",
    "    print(i, idx_max, idx, word, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolve\n",
    "1. Initialize a bag of scientists from a set of nodes.\n",
    "2. Mutate nodes. For each node,\n",
    "    1. Change a word with `p_point`. Draw weight from power law prior.\n",
    "    2. Delete a word with `p_delete`.\n",
    "    3. Insert new word with `p_insert`. Draw weight from power law prior.\n",
    "3. Crossover new nodes if `μ+3σ < similarity`.\n",
    "    1. Crossover nodes into one.\n",
    "4. Create new node if `x < similarity` where `x~Norm(θ)`.\n",
    "    1. Connect new node.\n",
    "    2. Initialize new scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(graph, tfidf, year_end=2020):\n",
    "    \"\"\"Evolves a graph based on tf-idf representations.\"\"\"\n",
    "    year_start = max([graph.nodes[n]['year'] for n in graph.nodes])+1\n",
    "    seed_nodes = {}\n",
    "    for year in range(year_start, year_start+1):#year_end+1):\n",
    "        for node in graph.nodes:\n",
    "            if node not in seed_nodes.keys():\n",
    "                seed_nodes[node] = tfidf[:,list(graph.nodes).index(node)].copy()\n",
    "        for node, vec in seed_nodes.items():\n",
    "            seed_nodes[node] = mutate(vec)\n",
    "#         vecs = hh.stack([])\n",
    "        # join seeds/crossover\n",
    "        for node, vec in seed_nodes.items():\n",
    "            if False:\n",
    "                graph.add_node(node)\n",
    "                # connect node\n",
    "                tfidf = ss.hstack([tfidf,vec])\n",
    "                seed_nodes[node] = None\n",
    "        print(year, '\\t', seed_nodes, '\\n')\n",
    "    return graph, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm(graph):\n",
    "    \"\"\"Grows a new graph with a core set of nodes selected from ``graph``.\n",
    "    Assumes ``graph.graph['tfidf']`` holds a ``scipy.sparse.csc_matrix`` of \n",
    "    tf-idf vectors.\n",
    "    \"\"\"\n",
    "    core = [n for n in graph.nodes if graph.nodes[n]['year'] < -2000]\n",
    "    tfidf = ss.hstack([graph.graph['tfidf'][:,list(graph.nodes).index(n)] for n in core])\n",
    "    subgraph = graph.subgraph(core).copy()\n",
    "    subgraph.graph.clear()\n",
    "    subgraph.name = graph.name + '-graft'\n",
    "    new_graph, new_tfidf = evolve(subgraph, tfidf)\n",
    "    new_graph.graph['tfidf'] = new_tfidf\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(graph.graph['tfidf'].sum(0))\n",
    "plt.title(graph.name)\n",
    "plt.xlabel('sum of tf-idf weights');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/37170511/scaled-logarithmic-binning-in-python\n",
    "plot_distribution(graph.graph['tfidf'].data)\n",
    "plt.title(graph.name)\n",
    "plt.xlabel('tf-idf weights');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    plot_distribution(graph.graph['tfidf'][:,i].data)\n",
    "    plt.title(list(graph.nodes)[i])\n",
    "    plt.xlabel('tf-idf weights');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check after mutation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
