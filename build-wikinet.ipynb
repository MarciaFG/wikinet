{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import mwparserfromhell as mph\n",
    "import re\n",
    "\n",
    "class WikiDump():\n",
    "    def __init__(self, path_xml, path_idx):\n",
    "        self._idx = {}\n",
    "        self._links = []\n",
    "        self._page = None\n",
    "        self.path_xml = path_xml\n",
    "        self.path_idx = path_idx\n",
    "        \n",
    "    def get_idx(self):\n",
    "        if self._idx:\n",
    "            return self._idx\n",
    "        else:\n",
    "            print('WikiDump: Loading index...')\n",
    "            with bz2.BZ2File(path_index, 'rb') as file:\n",
    "                lines = [line for line in file]\n",
    "            block_end = os.path.getsize(self.path_xml)\n",
    "            offset_prev = block_end\n",
    "            for line in reversed(lines):\n",
    "                [offset, pid, name] = line.strip().split(b':', 2)\n",
    "                offset, pid, name = (int(offset), int(pid), name.decode('utf8'))\n",
    "                block_end = offset_prev if offset < offset_prev else block_end\n",
    "                self._idx[name] = (offset, pid, block_end-offset)\n",
    "                offset_prev = offset\n",
    "            print('WikiDump: Loaded.')\n",
    "            return self._idx\n",
    "    idx = property(get_idx)\n",
    "    \n",
    "    def get_links(self):\n",
    "        if self._links:\n",
    "            return self._links\n",
    "        elif self.page:\n",
    "            self._links = [x.title for x in self.page.filter_wikilinks()]\n",
    "            return self._links\n",
    "    links = property(get_links)\n",
    "    \n",
    "    def get_page(self):\n",
    "        return self._page\n",
    "    \n",
    "    def set_page(self, page):\n",
    "        self._page = page\n",
    "        self._links = []\n",
    "    page = property(get_page, set_page)\n",
    "    \n",
    "    def load_page(self, page_name, filter_top=False):\n",
    "        if page_name not in self.idx.keys():\n",
    "            self.page = None\n",
    "            return\n",
    "        offset, pid, block_size = self.idx[page_name]\n",
    "        xml = WikiDump.fetch_block(self.path_xml, offset, block_size)\n",
    "        root = ET.fromstring(b'<root>' + xml + b'</root>')\n",
    "        text = WikiDump.search_id(root, pid)\n",
    "        text = WikiDump.filter_top_section(text) if filter_top else text\n",
    "        self.page = mph.parse(text)\n",
    "        return self.page\n",
    "    \n",
    "    @staticmethod\n",
    "    def fetch_block(path, offset, block_size):\n",
    "        with open(path, 'rb') as file:\n",
    "            file.seek(offset)\n",
    "            return bz2.decompress(file.read(block_size))\n",
    "    \n",
    "    @staticmethod\n",
    "    def search_id(root, pid):\n",
    "        for page in root.iter('page'):\n",
    "            if pid == int(page.find('id').text):\n",
    "                return page.find('revision').find('text').text\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_top_section(text):\n",
    "        head = re.search(r'==.*?==', text)\n",
    "        idx = head.span(0)[0] if head else len(text)\n",
    "        return text[:idx] #(text[:idx], text[idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = '/Users/harangju/Developer/data/wiki/'\n",
    "name_xml = 'enwiki-20190801-pages-articles-multistream.xml.bz2'\n",
    "name_index = 'enwiki-20190801-pages-articles-multistream-index.txt.bz2'\n",
    "path_xml = path_base + name_xml\n",
    "path_index = path_base + name_index\n",
    "dump = WikiDump(path_xml, path_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the wiki dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiDump: Loading index...\n",
      "WikiDump: Loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Classical physics', 'mechanics', 'optics', 'electricity', 'magnetism']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump.load_page('Portal:Physics/Topics')\n",
    "dump.links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plasma (physics)', 'quartz', 'solid', 'water', 'liquid']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump.load_page('Matter', filter_top=True)\n",
    "dump.links[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia hypernet traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "class WikiCrawler():\n",
    "    @staticmethod\n",
    "    def bfs(graph, dump, queue, depth_goal=1):\n",
    "        depth = 0\n",
    "        depth_num_items = len(queue)\n",
    "        depth_inc_pending = False\n",
    "        while queue:\n",
    "            page = queue.pop(0)\n",
    "            depth_num_items -= 1\n",
    "            if depth_num_items == 0:\n",
    "                depth += 1\n",
    "                print('Depth: ' + str(depth))\n",
    "                depth_inc_pending = True\n",
    "            if dump.load_page(page, filter_top=True):\n",
    "                for link in dump.links:\n",
    "                    link = str(link).split('#')[0].capitalize()\n",
    "                    if (page, link) not in graph.edges:\n",
    "                        graph.add_edge(link, page)\n",
    "                        queue.append(link)\n",
    "            if depth_inc_pending:\n",
    "                depth_num_items = len(queue)\n",
    "                depth_inc_pending = False\n",
    "            if depth == depth_goal:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.DiGraph()\n",
    "queue = ['Matter']\n",
    "WikiCrawler.bfs(graph, dump, queue, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webweb import Web\n",
    "\n",
    "edge_list = [e for e in graph.edges]\n",
    "Web(edge_list).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
