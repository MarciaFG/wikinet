{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the wiki dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wiki\n",
    "\n",
    "path_base = '/Users/harangju/Developer/data/wiki/'\n",
    "name_xml = 'enwiki-20190801-pages-articles-multistream.xml.bz2'\n",
    "name_index = 'enwiki-20190801-pages-articles-multistream-index.txt.bz2'\n",
    "path_xml = path_base + name_xml\n",
    "path_index = path_base + name_index\n",
    "dump = wiki.Dump(path_xml, path_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dump.load_page('Portal:Physics/Topics')\n",
    "dump.links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump.load_page('Danielle Bassett')\n",
    "dump.links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump.load_page('Matter', filter_top=True).strip_code()[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get index of physics articles\n",
    "\n",
    "* [all indices on Wikipedia](https://en.wikipedia.org/wiki/Portal:Contents/Indices)\n",
    "* topics not searched\n",
    "* international trade (\"topics\"), theory of constraints (small)\n",
    "* too big: mathematics, neuroscience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural & physical sciences\n",
    "topics = ['anatomy', 'biochemistry', 'cognitive science', 'evolutionary biology',\n",
    "          'genetics', 'immunology', 'molecular biology']\n",
    "topics += ['chemistry', 'biophysics', 'energy', 'optics', \n",
    "           'earth science', 'geology', 'meteorology']\n",
    "# philosophy\n",
    "# topics += []\n",
    "topics += ['philosophy of language', 'philosophy of law', \n",
    "           'philosophy of mind', 'philosophy of science']\n",
    "# social sciences\n",
    "topics += ['economics', 'accounting', 'education', 'linguistics', 'law', 'psychology', 'sociology']\n",
    "# technology & applied sciences\n",
    "topics += ['electronics', 'software engineering', 'robotics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = {}\n",
    "for topic in topics:\n",
    "    dump.load_page('Index of %s articles' % topic)\n",
    "    links[topic] = [str(l) for l in dump.article_links]\n",
    "    print('Topic \"' + topic + '\" has ' + str(len(links[topic])) + ' articles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "links['physics'] = []\n",
    "for letter in ['!$@', '0â€“9'] + list(string.ascii_uppercase):\n",
    "    dump.load_page('Index of physics articles (%s)' % letter)\n",
    "    links['physics'].extend([str(l) for l in dump.article_links])\n",
    "print('Topic \"' + 'physics' + '\" has ' + str(len(links['physics'])) + ' articles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graphs of topics\n",
    "\n",
    "### 1 network per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "path_to_save = '/Users/harangju/Box Sync/Research/my papers/wikipedia paper/data/graphs/undated/'\n",
    "\n",
    "graphs = {}\n",
    "page_noload = {}\n",
    "depth = 1\n",
    "for topic in links.keys():\n",
    "    print('Graph topic: ' + topic)\n",
    "    graphs[topic] = nx.DiGraph()\n",
    "    page_noload[topic] = wiki.Crawler.bfs(graphs[topic], dump, links[topic],\n",
    "                                          depth_goal = depth, nodes = links[topic])\n",
    "    nx.write_gexf(graphs[topic], path_to_save + topic + '.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic, graph in graphs.items():\n",
    "    graph.remove_nodes_from(nx.isolates(graph))\n",
    "    nx.write_gexf(graph, path_to_save + topic + '.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gephi notes\n",
    "* node size, fruchterman reingold = [10, 40], force atlas 2 = [4 16]\n",
    "* text size = [1 1.4]\n",
    "* preview font size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 network for all topics\n",
    "i.e., connect all networks into 1 big network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_list = [item for sublist in list(links.values()) for item in sublist]\n",
    "# big_graph = nx.DiGraph()\n",
    "# wiki.Crawler.bfs(big_graph, dump, big_list, depth_goal = 2, nodes = big_list)\n",
    "# nx.write_gexf(big_graph, path_base + 'graphs/big_graph.gexf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
