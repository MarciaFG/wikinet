{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "\n",
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            if self._current_tag == 'id' and self._buffer:\n",
    "                return\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('title', 'text', 'id'):\n",
    "            if name == 'id' and name in self._values.keys():\n",
    "                return\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import mwparserfromhell as mph\n",
    "import re\n",
    "\n",
    "class WikiEngine():\n",
    "    def __init__(self, path_xml, path_idx):\n",
    "        # path_xml is the path to the XML bz2 file\n",
    "        # path_idx is the path to the index bz2 file\n",
    "        self._idx = None\n",
    "        self._block_sizes = None\n",
    "        self._links = None\n",
    "        self.page = None\n",
    "        self.path_xml = path_xml\n",
    "        self.path_idx = path_idx\n",
    "        self.handler = WikiXmlHandler()\n",
    "        self.parser = xml.sax.make_parser()\n",
    "        self.parser.setContentHandler(self.handler)\n",
    "        \n",
    "    def get_idx(self):\n",
    "        if self._idx:\n",
    "            return self._idx\n",
    "        elif self.path_idx:\n",
    "            print('Loading index...')\n",
    "            self._idx = {}\n",
    "            with bz2.BZ2File(self.path_idx, 'rb') as file:\n",
    "                for line in file:\n",
    "                    [offset, page_id, name] = line.strip().split(b':', 2)\n",
    "                    self._idx[name.decode('utf-8')] = (int(offset), int(page_id))\n",
    "            return self._idx\n",
    "    idx = property(get_idx)\n",
    "    \n",
    "    def get_block_sizes(self):\n",
    "        if self._block_sizes:\n",
    "            return self._block_sizes\n",
    "        elif self._idx:\n",
    "            print('Calculating block sizes...')\n",
    "            offsets = [x[0] for x in self._idx.values()]\n",
    "            diff = [i-j for i, j in zip(offsets[1:], offsets[:-1])]\n",
    "            self._block_sizes = list(filter(lambda a: a != 0, diff))\n",
    "            return self._block_sizes\n",
    "    block_sizes = property(get_block_sizes)\n",
    "    \n",
    "    def get_links(self):\n",
    "        if self._links:\n",
    "            return self._links\n",
    "        elif self.page:\n",
    "            self._links = [x.title for x in self.page.filter_wikilinks()]\n",
    "            return self._links\n",
    "    links = property(get_links)\n",
    "    \n",
    "    def load_page(self, page_name):\n",
    "        if page_name not in self.idx.keys():\n",
    "            return\n",
    "        page_offset, page_id = self.idx[page_name]\n",
    "        print('Searching for page \"' + page_name + '\"'\n",
    "              ' with id ' + str(page_id) + '...')\n",
    "        xml = WikiEngine.search_dump(self.path_xml, page_id, page_offset).decode('utf-8')\n",
    "        xml = WikiEngine.strip_manual_ref(xml)\n",
    "        print('Loaded.')\n",
    "        print('Parsing XML...')\n",
    "        self.parser.feed(xml)\n",
    "        print('Parsing wiki (only the top section)...')\n",
    "#         text = WikiEngine.filter_top_section(self.handler._values['text'])\n",
    "        self.page = mph.parse(self.handler._values['text'])\n",
    "        print('Parsed.')\n",
    "        self._links = None\n",
    "        self.parser.reset()\n",
    "        return self.page\n",
    "    \n",
    "    @staticmethod\n",
    "    def fetch_dump(path, offset, block_size):\n",
    "        with open(path, 'rb') as file:\n",
    "            file.seek(offset)\n",
    "            data = file.read(block_size)\n",
    "            return bz2.decompress(data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def search_dump(path, page_id, offset, block_size):\n",
    "#         page_found = False\n",
    "#         xml = b''\n",
    "#         page_start = 0\n",
    "#         max_search = 100e6\n",
    "        with bz2.BZ2File(path, 'rb') as file:\n",
    "            file.seek(offset)\n",
    "            print(file.tell())\n",
    "            while (file.tell() - offset) < max_search:\n",
    "                line = file.readline()\n",
    "                if b'<page>' in line:\n",
    "                    xml = b''\n",
    "                    page_start = file.tell() - len(line)\n",
    "                xml = xml + line\n",
    "                if b'<id>' + str(page_id).encode('utf-8') + b'</id>' in line:\n",
    "                    print(page_start)\n",
    "#                     print('Found at byte offset ' + page_start + '.')\n",
    "                    page_found = True\n",
    "                if b'</page>' in line and page_found:\n",
    "                    return xml\n",
    "        raise NameError('Page not found.')\n",
    "    \n",
    "    @staticmethod\n",
    "    def strip_manual_ref(text):\n",
    "        return re.sub(r'&lt;/*ref.*?(/&gt;|&gt;)', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_top_section(text):\n",
    "        head = re.search(r'==.*?==', text)\n",
    "        idx = head.span(0)[0] if head else len(text)\n",
    "        return (text[:idx], text[idx:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/harangju/Developer/data/wiki/partition/'\n",
    "xml_name = 'enwiki-20190720-pages-articles-multistream1.xml-p10p30302.bz2'\n",
    "index_name = 'enwiki-20190720-pages-articles-multistream-index1.txt-p10p30302.bz2'\n",
    "xml_path = base_path + xml_name\n",
    "index_path = base_path + index_name\n",
    "wiki = WikiEngine(xml_path, index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.block_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.load_page('AccessibleComputing')\n",
    "# wiki.load_page('Artificial languages')\n",
    "# wiki.load_page('Abstract (law)')\n",
    "# wiki.load_page('Anxiety')\n",
    "# wiki.load_page('Foreign relations of Azerbaijan')\n",
    "# wiki.load_page('Alfonso Cuar√≥n')\n",
    "# wiki.load_page('ADHD')\n",
    "print('Number of links: ' + str(len(wiki.links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "x[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(xml_path, 'rb') as file:\n",
    "    file.seek(617)\n",
    "    data = file.read(1000)\n",
    "text = bz2.decompress(data)\n",
    "text[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
